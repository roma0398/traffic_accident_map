{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import AddressExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stop_words = stopwords.words(\"russian\")\n",
    "\n",
    "def process_text(text):\n",
    "    lower = (word.lower() for word in nltk.wordpunct_tokenize(text))\n",
    "    pattern = re.compile(\"[–∞-—è+0-9-]+\")\n",
    "    words = pattern.findall(\" \".join(lower))\n",
    "    norm_form = (morph.parse(word)[0].normal_form for word in words)\n",
    "    tokens = [word for word in text if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stop_words = stopwords.words(\"russian\")\n",
    "\n",
    "def process_text(text):\n",
    "    tok = text.split(' ')\n",
    "    pattern = re.compile(\"[–ê-—è+0-9-]+\")\n",
    "    words = pattern.findall(\" \".join(tok))\n",
    "    #norm_form = (stemmer.stem(word) for word in words)\n",
    "    tokens = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/post_data.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '–ú–æ–π–∫–∞ –Ω–∞ –ü–õ–ê–ù–ï–†–ù–û–ô –æ–±—ä—è–≤–ª—è–µ—Ç –æ–± –ê–ö–¶–ò–Ø–• :\\n\\n‚ú®–ü–æ–∫—Ä—ã—Ç–∏–µ –¢–µ—Ñ–ª–æ–Ω (–∑–∞—â–∏—Ç–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –±–ª–µ—Å–∫+–≥–∏–¥—Ä–æ—Ñ–æ–±)\\n–°–µ–¥–∞–Ω 1 000 —Ä—É–±–ª–µ–π\\n–ö—Ä–æ—Å—Å–æ–≤–µ—Ä 1 250 —Ä—É–±–ª–µ–π\\n–î–∂–∏–ø 1 500 —Ä—É–±–ª–µ–π\\n(–í —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤–∫–ª—é—á–µ–Ω–∞ –º–æ–π–∫–∞ –∞–≤—Ç–æ, –æ–±–µ–∑–∂–∏—Ä–∏–≤–∞–Ω–∏–µ, –ø–æ–ª–∏–º–µ—Ä–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ —Å —Ç–µ—Ñ–ª–æ–Ω–æ–º)\\n–ê–∫—Ç—É–∞–ª—å–Ω–æ –≤ –¥–æ–∂–¥—å, —Ç.–∫. –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫–∏–π –≥–∏–¥—Ä–æ—Ñ–æ–±–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç –∏ –º–∞—à–∏–Ω–∞ –¥–æ–ª—å—à–µ –æ—Å—Ç–∞—ë—Ç—Å—è —á–∏—Å—Ç–æ–π!!!\\n\\n‚ú® –∑–∞—â–∏—Ç–Ω–æ–µ –≥–∏–¥—Ä–æ—Ñ–æ–±–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–ª—è —Å—Ç—ë–∫–æ–ª\\n–õ–æ–±–æ–≤–æ–µ —Å—Ç–µ–∫–ª–æ 1 200 —Ä—É–±–ª–µ–π\\n–õ–æ–±–æ–≤–æ–µ —Å—Ç–µ–∫–ª–æ\\n+ –ø–µ—Ä–µ–¥–Ω–∏–µ –±–æ–∫–æ–≤—ã–µ 1 500 —Ä—É–±–ª–µ–π\\n(–ò–º–µ–µ—Ç —Å–∏–ª—å–Ω—ã–π –≥–∏–¥—Ä–æ—Ñ–æ–±–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç,\\n—á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–µ –≤–æ–∂–¥–µ–Ω–∏–µ –≤ —É—Å–ª–æ–≤–∏—è—Ö –¥–æ–∂–¥—è! –°—Ä–æ–∫ —Å–ª—É–∂–±—ã 6 –º–µ—Å—è—Ü–µ–≤! )\\n\\n‚ú®–ú–æ–π–∫–∞ –õ–Æ–ö–°\\n–°–µ–¥–∞–Ω/–∫—Ä–æ—Å—Å–æ–≤–µ—Ä 500 —Ä—É–±\\n–î–∂–∏–ø 700 —Ä—É–±\\n(–í —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤—Ö–æ–¥–∏—Ç –º–æ–π–∫–∞ –°–¢–ê–ù–î–ê–†–¢, –ø—ã–ª–µ—Å–æ—Å —Å–∞–ª–æ–Ω–∞, –ø—Ä–æ—Ç–∏—Ä–∞–Ω–∏–µ –≤—Å–µ—Ö –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π, –Ω–∞–Ω–µ—Å–µ–Ω–∏–µ –ø–æ–ª–∏—Ä–æ–ª–µ–π –¥–ª—è –ø–ª–∞—Å—Ç–∏–∫–∞, –º–æ–π–∫–∞ —Å—Ç—ë–∫–æ–ª )\\n\\n–ù–∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–∏–¥—ã —Ä–∞–±–æ—Ç —É—Ç–æ—á–Ω—è–π—Ç–µ —Ü–µ–Ω—ã –ø–æ —Ç–µ–ª: 8-921-750-35-86 –∏–ª–∏ –≤\\n–ª–∏—á–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è https://vk.com/milamaier\\n\\nüìç–ê–¥—Ä–µ—Å: –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, —É–ª. –ü–ª–∞–Ω–µ—Ä–Ω–∞—è, 24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('street_list_spb.txt', 'r')\n",
    "l = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "o = []\n",
    "for i in list(df['text']):\n",
    "    sample = process_text(i)\n",
    "    p = []\n",
    "    for j in l:\n",
    "        if sample.find(stemmer.stem(j)) > 0:\n",
    "            p.append(j)\n",
    "    o.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n"
     ]
    }
   ],
   "source": [
    "d = 0\n",
    "for i in o:\n",
    "    if len(i)>0:\n",
    "        d+=1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address(parts=[Settlement(name='–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥', type=None), Street(name='–ü–ª–∞–Ω–µ—Ä–Ω–∞—è', type='—É–ª–∏—Ü–∞'), Building(number='24', type=None)])\n"
     ]
    }
   ],
   "source": [
    "extractor = AddressExtractor()\n",
    "sample = process_text(text)\n",
    "matches = extractor(process_text(sample))\n",
    "for match in matches:\n",
    "    print(match.fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1352d3c310747c4b19cc60f7c80d66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58123), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 0\n",
    "f = open('street.txt', 'w')\n",
    "extractor = AddressExtractor()\n",
    "for i in tqdm(list(df['text'])):\n",
    "    sample = process_text(i)\n",
    "    matches = extractor(sample)\n",
    "    for match in matches:\n",
    "        s = ''\n",
    "        for j in range(0, len(match.fact.parts)):\n",
    "            try:\n",
    "                if match.fact.parts[j].type is None:\n",
    "                    try:\n",
    "                        s += match.fact.parts[j].number\n",
    "                        continue\n",
    "                    except:\n",
    "                        continue\n",
    "                try:\n",
    "                    if match.fact.parts[j].number is None:\n",
    "                        break\n",
    "                    s += match.fact.parts[j].type + ' ' + match.fact.parts[j].number + ' '\n",
    "                except:\n",
    "                    if match.fact.parts[j].name is None:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "                s += match.fact.parts[j].type + ' ' + match.fact.parts[j].name + ' '\n",
    "        f.write(s + '\\n')\n",
    "    if len(matches)>0:\n",
    "        k += 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ù–∞ –ö–æ–º—Å–æ–º–æ–ª–∞ —É–ª–∏—Ü–µ –¥–æ–º–∞ 10 —É–µ—Ö–∞–ª —Å–≤–µ—Ç–æ—Ñ–æ—Ä –≤—Ç–æ—Ä–æ–π —Å–µ—Ä–µ–¥–∏–Ω–µ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–∞'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = AddressExtractor()\n",
    "matches = extractor(process_text(list(df['text'])[0]))\n",
    "for match in matches:\n",
    "    print(match.span, match.fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Street(name='–ö–æ–º—Å–æ–º–æ–ª–∞',\n",
       "        type='—É–ª–∏—Ü–∞'), Building(number='10',\n",
       "          type='–¥–æ–º')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.fact.parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = match.fact.parts[0].name + ' ' + match.fact.parts[0].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-7d845714cd49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(match.fact.parts)):\n",
    "    if match.fact.parts[i].type is None:\n",
    "        s += match.fact.parts[i].number\n",
    "    else:\n",
    "        s += match.fact.parts[i].type + ' ' + match.fact.parts[i].number + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–õ–∏–≥–æ–≤—Å–∫–æ–º –ø—Ä–æ—Å–ø–µ–∫—Ç'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.fact.parts[2].type is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
